{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit 12 Week 2 Activity 2 Stu_CNN\n",
    "Create html string, then parse it into a BeautifulSoup object and print a formatted version of the soup\n",
    "\n",
    "#### Unit 12 Week 2 Activity 3 Ins_Craiglist\n",
    "Create variable for url of page to be scraped, retrieve page with the requests module, parse with html.parser, examine the results (with soup.prettify()) and determine the element with the sought after info, return results as an iterable list and loop thru the returned results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module used to connect Python with MongoDb\n",
    "import pymongo\n",
    "# Import BeautifulSoup\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# Import Requests module\n",
    "import requests\n",
    "# Import splinter\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default port used by MongoDB is 27017\n",
    "# https://docs.mongodb.com/manual/reference/default-mongodb-port/\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'classDB' database in Mongo\n",
    "db = client.classDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape NASA MARS NEWS from\n",
    "`https://mars.nasa.gov/news/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: URL of page to be scraped\n",
    "url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Retrieve page with the requests module\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Examine the results, then determine element that contains sought info\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: results are returned as an iterable list\n",
    "results = soup.find_all(\"div\", id=\"page\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = result.find('ul', class_=\"grid_gallery\")\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Loop through returned results\n",
    "for result in results:\n",
    "    # Error handling\n",
    "    try:\n",
    "        # Identify and return news title\n",
    "        news_date = result.find('div', class_=\"list_date\").text\n",
    "        # Identify and return news title\n",
    "        news_title = result.find('div', class_=\"content_title\").text\n",
    "        # Identify and return paragraph text\n",
    "        news_p = result.find('div', class_=\"article_teaser_body\").text\n",
    "        \n",
    "        if (news_list and news_date and news_title and news_p):\n",
    "            print('------')\n",
    "            print(news_list)\n",
    "            print(news_date)\n",
    "            print(news_title)\n",
    "            print(news_p)\n",
    "    except AttributeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "#### Scrape JPL Mars Space Images\n",
    "\n",
    "- JPL Mars Space Images - Featured Image\n",
    "- Visit the url for JPL Featured Space Image [here](https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars).\n",
    "- Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called `featured_image_url`.\n",
    "- Make sure to find the image url to the full size `.jpg` image.\n",
    "- Make sure to save a complete url string for this image.\n",
    "\n",
    "##### Example:\n",
    "`featured_image_url = 'https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA16225_hires.jpg'`\n",
    "\n",
    "#### Unit 12 Week 2 Activity 08 Stu_Splinter_Advanced\n",
    "http://localhost:8888/notebooks/OneDrive/Documents/GitHub/12.0-Web-Scraping-and-Document-Databases/2/Activities/08-Stu_Splinter/Solved/Stu_Splinter_Advanced.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Driver [C:\\Users\\diane\\.wdm\\drivers\\chromedriver\\win32\\87.0.4280.88\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "# Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24278_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24277_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24276_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24275_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24274_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24273_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24272_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24271_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24260_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24259_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24258_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24257_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24256_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24255_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24254_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24253_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24252_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24251_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24250_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24249_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24248_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24247_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24246_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24245_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24244_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24173_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24243_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24242_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24241_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24190_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24189_hires.jpg',\n",
       " 'https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA24188_hires.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "articles = soup.find('ul', class_='articles')\n",
    "\n",
    "slides = articles.find_all('li')\n",
    "\n",
    "url_list = []\n",
    "\n",
    "for slide in slides:\n",
    "    featured_image_url = slide.find('a')['data-fancybox-href']\n",
    "    url_list.append(featured_image_url)\n",
    "    \n",
    "url_list = ['https://www.jpl.nasa.gov/' + url for url in url_list]\n",
    "\n",
    "url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "#### Scrape Mars Facts\n",
    "- Visit the Mars Facts webpage [here](https://space-facts.com/mars/) and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "- Use Pandas to convert the data to a HTML table string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: URL of page to be scraped\n",
    "url = 'https://space-facts.com/mars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Retrieve page with the requests module\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Examine the results, then determine element that contains sought info\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: results are returned as an iterable list\n",
    "results = soup.find_all('div', class_=\"slide\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Loop through returned results\n",
    "for result in results:\n",
    "    # Error handling\n",
    "    try:\n",
    "        # Identify and return news title\n",
    "        news_title = result.find('div', class_=\"content_title\").text\n",
    "        # Identify and return paragraph text\n",
    "        news_p = result.find('div', class_=\"rollover_description_inner\").text\n",
    "        \n",
    "        if (news_title and news_p):\n",
    "            print('------')\n",
    "            print(news_title)\n",
    "            print(news_p)\n",
    "    except AttributeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape USGS Astrogeology/Mars Hemispheres\n",
    "\n",
    "- Visit the USGS Astrogeology site [here](https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) to obtain high resolution images for each of Mar's hemispheres.\n",
    "- You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "- Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys `img_url` and `title`.\n",
    "- Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere.\n",
    "\n",
    "\n",
    "##### Example:\n",
    "hemisphere_image_urls = [\n",
    "    {\"title\": \"Valles Marineris Hemisphere\", \"img_url\": \"...\"},\n",
    "    {\"title\": \"Cerberus Hemisphere\", \"img_url\": \"...\"},\n",
    "    {\"title\": \"Schiaparelli Hemisphere\", \"img_url\": \"...\"},\n",
    "    {\"title\": \"Syrtis Major Hemisphere\", \"img_url\": \"...\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: URL of page to be scraped\n",
    "url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Retrieve page with the requests module\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Examine the results, then determine element that contains sought info\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: results are returned as an iterable list\n",
    "results = soup.find_all('div', class_=\"slide\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Loop through returned results\n",
    "for result in results:\n",
    "    # Error handling\n",
    "    try:\n",
    "        # Identify and return news title\n",
    "        news_title = result.find('div', class_=\"content_title\").text\n",
    "        # Identify and return paragraph text\n",
    "        news_p = result.find('div', class_=\"rollover_description_inner\").text\n",
    "        \n",
    "        if (news_title and news_p):\n",
    "            print('------')\n",
    "            print(news_title)\n",
    "            print(news_p)\n",
    "    except AttributeError as e:\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
